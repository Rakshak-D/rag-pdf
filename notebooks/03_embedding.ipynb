{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding for images",
   "id": "28200c49c55cc82a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:32.949539100Z",
     "start_time": "2026-01-28T14:42:26.203101400Z"
    }
   },
   "source": [
    "import numpy as np # Used for storing embeddings .\n",
    "import torch # For device selection , model execution and tensor operation .\n",
    "from PIL import Image # Used for image creation and other image operation .\n",
    "import open_clip # Image embedding model .\n",
    "from typing import List, Dict # Used for type return ."
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:32.963856300Z",
     "start_time": "2026-01-28T14:42:32.949539100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Used for loading embedding model and embedding images with their caption .\n",
    "class ImageEmbeddingModel:\n",
    "    def __init__(self,model_name:str=\"ViT-B-32\",pretrained:str=\"laion2b_s34b_b79k\"):\n",
    "\n",
    "        self.device=\"cuda\" if torch.cuda.is_available() else \"cpu\" # Used to check if the machine has GPU if not assign CPU as device for working .\n",
    "        self.model,self.preprocess,self.tokenizer=open_clip.create_model_and_transforms(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained\n",
    "        ) # Getting essential function from model .\n",
    "\n",
    "        self.model=self.model.to(self.device) # Device selection for model operations .\n",
    "        self.model.eval() # Loading model\n",
    "\n",
    "        # Displaying device on which model will run .\n",
    "        if self.device==\"cuda\":\n",
    "            print(f\"OpenClip running on {torch.cuda.get_device_name(0)} .\")\n",
    "        else:\n",
    "            print(\"OpenClip running on CPU .\")\n",
    "\n",
    "    # Using no grad since by default torch assumes it is used for training model and do various backpropagation tasks which are not required here and only forward pass is required .\n",
    "    @torch.no_grad()\n",
    "    def embed_image(self,image_objects:List[Dict])->List[np.ndarray]:\n",
    "        if not image_objects:\n",
    "            raise ValueError(\"No images in the image object .\")\n",
    "        image_objects_embeddings=[] # Used for storing image embeddings .\n",
    "\n",
    "        for image_object in image_objects: # Iterating through image objects .\n",
    "            image_path=image_object[\"path\"] # Fetching location of image.\n",
    "            image=Image.open(image_path).convert(\"RGB\") # Converting RGB if image is not already in RGB .\n",
    "            image_tensor=self.preprocess(image).unsqueeze(0).to(self.device) # Converting into tensor since openclip cant embedd other inputs .\n",
    "\n",
    "            emb_imag=self.model.encode_image(image_tensor) # Embedding images\n",
    "            emb_imag=emb_imag/emb_imag.norm(dim=-1,keepdim=True) # Normalizing images\n",
    "\n",
    "            emb_imag=emb_imag.cpu().numpy()[0] # Converting tensor output to numpy array . Using .cpu since numpy cannot access gpu memory ,it should be in ram memory then numpy can access memory and can convert it into numpy array .\n",
    "\n",
    "            if image_object[\"caption_text\"]: # If caption is available for the image then embedd it .\n",
    "                caption=image_object[\"caption_text\"]\n",
    "                tokens=open_clip.tokenize([caption]).to(self.device) # Used to convert into tensor .\n",
    "\n",
    "                emb_text=self.model.encode_text(tokens) # Embedding text or tensor value .\n",
    "                emb_text=emb_text/emb_text.norm(dim=-1,keepdim=True) # Normalizing values\n",
    "\n",
    "                emb_text=emb_text.cpu().numpy()[0] # Converting tensor to numpy array .\n",
    "\n",
    "                fused=emb_imag+emb_text # Fusing image and caption together .It does not change the meaning .\n",
    "\n",
    "                norm= np.linalg.norm(fused)\n",
    "                if norm>0:\n",
    "                    fused=fused / norm # Normalizing fused values .\n",
    "\n",
    "                image_objects_embeddings.append(fused) # Appending embeddings .\n",
    "\n",
    "            else:\n",
    "                image_objects_embeddings.append(emb_imag) # If no captions only images are embedded and appended .\n",
    "\n",
    "        return image_objects_embeddings # Returning all the embeddings ."
   ],
   "id": "cb44b3e5d53861bb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.410561Z",
     "start_time": "2026-01-28T14:42:32.964863600Z"
    }
   },
   "cell_type": "code",
   "source": "image_model = ImageEmbeddingModel() # Loading model .",
   "id": "2bd65869b3e99586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenClip running on NVIDIA GeForce RTX 3050 6GB Laptop GPU .\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.515610900Z",
     "start_time": "2026-01-28T14:42:34.500609500Z"
    }
   },
   "cell_type": "code",
   "source": "from PIL import ImageDraw # Used to create a image .",
   "id": "2cc4bdd3c76152",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.541848900Z",
     "start_time": "2026-01-28T14:42:34.516618100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = Image.new(\"RGB\", (400, 300), \"white\") # Creating a blank image .\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "draw.rectangle([50, 50, 350, 250], outline=\"black\", width=3) # Creating a triangle .\n",
    "draw.text((120, 140), \"Bias Variance\", fill=\"black\") # A text inside a box\n",
    "\n",
    "img.save(\"../data/test_images/test_image.png\") # Saving image .\n",
    "print(\"Saved test_image.png\")"
   ],
   "id": "8e7d36350a8498eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test_image.png\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.562463700Z",
     "start_time": "2026-01-28T14:42:34.542847200Z"
    }
   },
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt # For displaying image .",
   "id": "91f38b5c4f32472e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.627030300Z",
     "start_time": "2026-01-28T14:42:34.563463600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Displaying test image .\n",
    "test_image=Image.open(\"../data/test_images/test_image.png\")\n",
    "plt.imshow(test_image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Test image\")\n",
    "plt.show()"
   ],
   "id": "bfed98beee470dd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.640031900Z",
     "start_time": "2026-01-28T14:42:34.628030800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a test image object .\n",
    "image_objects = [\n",
    "    {\n",
    "        \"image_id\": \"test_img_1\",\n",
    "        \"path\": \"test_image.png\",\n",
    "        \"caption_text\": \"Bias variance tradeoff diagram\",\n",
    "        \"source\": \"unit_test\",\n",
    "        \"page_num\": 1,\n",
    "        \"bbox\": [0, 0, 400, 300]\n",
    "    }\n",
    "]"
   ],
   "id": "d532f8b38362562f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.856795400Z",
     "start_time": "2026-01-28T14:42:34.641030800Z"
    }
   },
   "cell_type": "code",
   "source": "image_embeddings = image_model.embed_image(image_objects) # Creating embeddings .",
   "id": "cc5077582e71cb9b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.877835600Z",
     "start_time": "2026-01-28T14:42:34.857793400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Details about image embedding .\n",
    "print(\"Number of embeddings:\", len(image_embeddings))\n",
    "print(\"Embedding shape:\", image_embeddings[0].shape)\n",
    "print(\"Embedding L2 norm:\", np.linalg.norm(image_embeddings[0]))"
   ],
   "id": "e23fcb15ea480d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 1\n",
      "Embedding shape: (512,)\n",
      "Embedding L2 norm: 1.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:34.898835300Z",
     "start_time": "2026-01-28T14:42:34.878836200Z"
    }
   },
   "cell_type": "code",
   "source": "image_embeddings[0][:100] # Displaying only first 100 vectors of image .",
   "id": "fa2a0d610cd49563",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.1581692e-02,  6.8287939e-02,  1.1431051e-02, -4.9217690e-02,\n",
       "       -1.5946979e-02,  2.4762062e-02,  2.6639581e-02, -3.2111760e-02,\n",
       "        1.3238541e-02,  3.1832445e-02, -2.2001790e-03, -2.5972907e-02,\n",
       "       -2.4729891e-02,  1.6912680e-02,  2.7349034e-02, -1.9847481e-03,\n",
       "       -2.4579107e-03, -2.8936736e-02,  2.0858616e-02,  3.9522253e-02,\n",
       "       -6.1152205e-03, -1.8011864e-02, -4.6620391e-02, -1.7663253e-02,\n",
       "        2.8710078e-02,  3.6942374e-02,  3.8816661e-02,  2.4634130e-02,\n",
       "        5.8818422e-03, -4.2670436e-02,  5.7039458e-02,  2.3086859e-02,\n",
       "       -8.6038947e-02,  3.3985700e-02,  7.6020830e-03,  5.2327286e-02,\n",
       "        4.9606904e-02, -3.8741890e-02,  3.4758575e-02,  1.3869371e-02,\n",
       "       -1.7099325e-02,  5.6585610e-02,  1.3118730e-02,  1.5918035e-02,\n",
       "       -6.2628286e-03, -1.4060762e-02, -2.7199015e-03, -9.7329300e-03,\n",
       "        4.7614291e-02,  1.8487448e-02,  1.4838857e-02, -2.8593015e-02,\n",
       "       -2.4045022e-02, -1.0945432e-02,  2.7024848e-02, -1.2386700e-02,\n",
       "        2.9497569e-02, -4.5492575e-02, -9.7693941e-03,  7.8126239e-03,\n",
       "        8.0362028e-03,  3.8259838e-02, -3.8623475e-02,  5.0431363e-02,\n",
       "       -1.6078331e-02,  1.1727479e-02,  9.9761914e-03, -5.4231533e-03,\n",
       "       -3.0440185e-02,  3.6605947e-02, -3.5781151e-01,  1.7905133e-02,\n",
       "        2.9055705e-02,  5.1934805e-02,  9.5011424e-03, -7.5823762e-02,\n",
       "       -1.1033098e-04, -2.6533207e-02,  8.1055444e-03, -9.7466353e-03,\n",
       "       -2.2503410e-02, -6.3465342e-02,  1.6712794e-02, -1.2530753e-02,\n",
       "        3.9876044e-02,  4.4899188e-02, -3.3433471e-02, -3.6334340e-02,\n",
       "        3.2544289e-02,  8.9225648e-03,  2.5675844e-02,  2.7356328e-02,\n",
       "       -3.2158580e-02, -2.5780795e-02,  1.7046273e-02, -3.2796629e-02,\n",
       "        6.9454787e-03, -1.0540548e-01, -5.3687254e-03,  3.9460994e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding for text",
   "id": "678e92ee3de67438"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:35.371416400Z",
     "start_time": "2026-01-28T14:42:34.899836200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer # Used for loading model .\n",
    "import numpy as np # Used to store embedding .\n",
    "import torch # Used for device selection and model execution .\n",
    "from typing import List # Used for return type .\n",
    "from langchain_core.documents import Document # Used for storing documents ."
   ],
   "id": "d7a549062a9feeea",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:35.386581600Z",
     "start_time": "2026-01-28T14:42:35.372412300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Used for loading model and embedding text .\n",
    "class TextEmbeddingModel:\n",
    "\n",
    "    def __init__(self,model_name:str= \"BAAI/bge-base-en-v1.5\"):\n",
    "        self.device=\"cuda\" if torch.cuda.is_available() else \"cpu\" # Device check .\n",
    "        self.model=SentenceTransformer(model_name_or_path=model_name,device=self.device) # Loading model .\n",
    "\n",
    "        if self.device==\"cuda\":\n",
    "            print(f\"BGE running on {torch.cuda.get_device_name(0)} .\")\n",
    "        else:\n",
    "            print(\"BGE running on CPU .\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_documents(self,documents:List[Document])->List[np.ndarray]:\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents to embed .\")\n",
    "\n",
    "        texts=[doc.page_content for doc in documents]\n",
    "\n",
    "        # Embedding texts .\n",
    "        # sentences -> input /texts\n",
    "        # batch_size -> Number of inputs embedding a single time .\n",
    "        # convert_to_numpy -> Convert output to numpy array .\n",
    "        # normalize_embeddings -> Normalizing all the out vectors .\n",
    "        text_embeddings=self.model.encode(\n",
    "            sentences=texts,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        return text_embeddings # Return embeddings .\n"
   ],
   "id": "e34007e72a751c5f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:39.283566Z",
     "start_time": "2026-01-28T14:42:35.388589100Z"
    }
   },
   "cell_type": "code",
   "source": "text_model = TextEmbeddingModel() # Loading model .",
   "id": "e97d5a3d0b4f3d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE running on NVIDIA GeForce RTX 3050 6GB Laptop GPU .\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:39.315454900Z",
     "start_time": "2026-01-28T14:42:39.299142100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test data\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The bias variance tradeoff explains the balance between underfitting and overfitting in machine learning.\",\n",
    "        metadata={\"chunk_id\": \"c1\", \"page_num\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Overfitting occurs when a model learns noise in the training data instead of the underlying pattern.\",\n",
    "        metadata={\"chunk_id\": \"c2\", \"page_num\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Regularization techniques such as L2 penalty help reduce overfitting by constraining model complexity.\",\n",
    "        metadata={\"chunk_id\": \"c3\", \"page_num\": 2}\n",
    "    )\n",
    "]"
   ],
   "id": "fa7d99b88f8d256d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:39.361458800Z",
     "start_time": "2026-01-28T14:42:39.315454900Z"
    }
   },
   "cell_type": "code",
   "source": "text_embeddings = text_model.embed_documents(documents) # Passing data or embedding input .",
   "id": "3b7fbd0dfe591e56",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:39.389455300Z",
     "start_time": "2026-01-28T14:42:39.363484400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Details of embeddings\n",
    "print(\"Number of embeddings:\", len(text_embeddings))\n",
    "print(\"Embedding shape:\", text_embeddings[0].shape)\n",
    "print(\"Embedding L2 norm:\", np.linalg.norm(text_embeddings[0]))"
   ],
   "id": "45af80e1732b609c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 3\n",
      "Embedding shape: (768,)\n",
      "Embedding L2 norm: 1.0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:42:39.410457400Z",
     "start_time": "2026-01-28T14:42:39.390455800Z"
    }
   },
   "cell_type": "code",
   "source": "text_embeddings[0][:100] # Embeddings of first document",
   "id": "5e66b3cae3975ccf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00750077, -0.01657214,  0.02134528,  0.03737882,  0.00916659,\n",
       "        0.00445431,  0.0467957 ,  0.0081098 , -0.02130136, -0.04648071,\n",
       "        0.06370625,  0.0443725 , -0.07843439, -0.01442318, -0.02948426,\n",
       "        0.03460341,  0.030627  ,  0.00833811,  0.03319671,  0.00328724,\n",
       "       -0.00491366, -0.00731131,  0.0147004 , -0.00487739,  0.00527096,\n",
       "        0.00481392,  0.01197181, -0.01464292, -0.01389897,  0.06801087,\n",
       "        0.07143721,  0.02321669, -0.04086312, -0.00409608, -0.01635015,\n",
       "        0.01832462, -0.00466213, -0.03060478, -0.01251492,  0.03860732,\n",
       "       -0.05658573,  0.02579218, -0.02520276,  0.03879949, -0.06607064,\n",
       "       -0.00870941, -0.12635188,  0.01515684, -0.02914351, -0.08872724,\n",
       "       -0.07440807,  0.03215913, -0.02752695,  0.00591359,  0.00066573,\n",
       "        0.03919731,  0.0353061 , -0.02116826, -0.03174032, -0.02149748,\n",
       "       -0.01491818,  0.03312138,  0.01685498, -0.02782962,  0.00900965,\n",
       "        0.02998906, -0.00292024,  0.03930453, -0.06367826, -0.04664085,\n",
       "       -0.04486695,  0.03225312, -0.01387275, -0.02046258, -0.0175173 ,\n",
       "       -0.02997852,  0.0102389 ,  0.01956647,  0.09625775,  0.00379656,\n",
       "       -0.00898061,  0.01198702,  0.0570937 ,  0.05778944,  0.01650331,\n",
       "       -0.02708699,  0.00578863, -0.02790647, -0.07690144,  0.07769017,\n",
       "        0.01640324, -0.05146175,  0.04066606,  0.04766781, -0.02937419,\n",
       "       -0.00067594,  0.03345395, -0.04927773, -0.00068618,  0.0343081 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
