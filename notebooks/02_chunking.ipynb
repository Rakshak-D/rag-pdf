{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Document loading",
   "id": "f64cafe3ef3f892d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:14.996649800Z",
     "start_time": "2026-01-26T16:52:14.809883200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymupdf # For pdf loading\n",
    "from langchain_core.documents import Document # For converting elements into Document .\n",
    "from pathlib import Path # For Loading the directory or PDF path .\n",
    "from typing import List\n",
    "import traceback # Used for error handling ."
   ],
   "id": "134471eb1010c65a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.012168800Z",
     "start_time": "2026-01-26T16:52:14.997655600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load all the PDF's from Directory .\n",
    "def loading_pdf(dir_path:str='../data/pdf')->List[Document]: # Return type .\n",
    "    dir_path=Path(dir_path) # Loading directory Path\n",
    "\n",
    "    if not dir_path.is_dir(): # Checking if the directory is valid .\n",
    "        raise NotADirectoryError(f\"{dir_path} is a invalid directory .\")\n",
    "\n",
    "    print(f\"The directory path is : {dir_path} .\")\n",
    "    docs=list(dir_path.rglob(\"*.pdf\")) # Storing all the PDF's path into list .\n",
    "    print(f\"Number of PDF's in directory is {len(docs)}\")\n",
    "\n",
    "    if len(docs)==0: # Checking if any PDFs exists in the directory .\n",
    "        print('No documents in the dir_path')\n",
    "        return []\n",
    "\n",
    "    # All these variables used for stats check at the end .\n",
    "    all_pdf_size=0.0\n",
    "    all_documents=[]\n",
    "    failed_pdf=[]\n",
    "\n",
    "    print(\"=\"*20,\"PDF LOAD SUMMARY\",\"=\"*20)\n",
    "    print(\"-\"*45)\n",
    "\n",
    "    for serial,pdf_path in enumerate(docs): # Iterating through all the PDFs in directory .\n",
    "        print(f\"{serial+1} ---> Loading {pdf_path.name} \")\n",
    "        pdf_size_bytes=pdf_path.stat().st_size\n",
    "        pdf_size_mb=pdf_size_bytes/(1024**2) # Calculating size of the PDF .\n",
    "        print(f\"File size : {pdf_size_mb:.3f} MB\")\n",
    "        try:\n",
    "            imag_dir= Path('../data/images_pymupdf') / pdf_path.stem # Directory for storing images in the PDF .\n",
    "            imag_dir.mkdir(parents=True,exist_ok=True)\n",
    "            pdf=pymupdf.open(filename=pdf_path,filetype=\"pdf\") # Loading PDF .\n",
    "\n",
    "            # The following function is to remove smaller texts which are not much useful while chunking or embedding .\n",
    "            def flush(page_num)->None:\n",
    "\n",
    "                if not text_blocks:\n",
    "                    return\n",
    "                document=Document(\n",
    "                    page_content=\"\\n\".join(b[\"text\"] for b in text_blocks), # Page content for the Document .\n",
    "                    metadata={\n",
    "                    'source':pdf_path.name,\n",
    "                    'page_num':page_num,\n",
    "                    'text_blocks':text_blocks.copy(),\n",
    "                    'images':page_images.copy(),\n",
    "                    }\n",
    "                    ) # Relevant metadata .\n",
    "\n",
    "                all_documents.append(document) # Storing Documents for next process .\n",
    "                text_blocks.clear()\n",
    "\n",
    "            for page_num,page in enumerate(pdf,start=1):\n",
    "                text_blocks=[] # Used for storing details about block of a page .\n",
    "                page_images=[] # Used for storing images of current page .\n",
    "\n",
    "                for img_index,img in enumerate(page.get_images(full=True)): # Extracting images .\n",
    "\n",
    "                    if img[1]!=0: # Removing smask . smask -> Transparency layer\n",
    "                        continue\n",
    "                    xref=img[0]\n",
    "\n",
    "                    rects=page.get_image_rects(xref) # Used for getting image edges .\n",
    "\n",
    "                    if not rects: # Checking if coordinates or image is empty .\n",
    "                        continue\n",
    "\n",
    "                    pix=pymupdf.Pixmap(pdf,xref) # xref is used to find position if image in PDF .\n",
    "\n",
    "                    if pix.width<50 or pix.height<50: # Removing very tiny images .\n",
    "                        pix=None\n",
    "                        continue\n",
    "                    if pix.alpha and pix.samples is not None: # Removing fully transparent images .\n",
    "                        if max(pix.samples)==0:\n",
    "                            continue\n",
    "\n",
    "                    if pix.n>4: # If image is CMY color format converting it into RGB .\n",
    "                        pix=pymupdf.Pixmap(pymupdf.csRGB,pix)\n",
    "\n",
    "\n",
    "                    img_path=imag_dir/f\"page_{page_num}_img_{img_index}.png\" # Location for storing images in local disk .\n",
    "                    pix.save(img_path) # Saving images in local disk .\n",
    "                    pix=None\n",
    "\n",
    "                    for rect in rects:\n",
    "                        page_images.append({\n",
    "                            \"image_id\":f\"{pdf_path.stem}_p{page_num}_i{img_index}\",\n",
    "                            \"path\":str(img_path),\n",
    "                            \"page\":page_num,\n",
    "                            \"bbox\":[rect.x0,rect.y0,rect.x1,rect.y1]\n",
    "                        }) # For metadata .\n",
    "\n",
    "                # Following loop is to extract texts from a page .\n",
    "                blocks=sorted(page.get_text(\"blocks\"),key=lambda b:(b[1],b[0]))\n",
    "                for block_id,b in enumerate(blocks):\n",
    "                    x0,y0,x1,y1,text=b[:5] # Coordinates and text of text block .\n",
    "                    text=text.strip()\n",
    "                    if len(text) < 20: # If texts are smaller it is removed since smaller texts can not be very useful .\n",
    "                        continue\n",
    "                    block_bbox=[x0,y0,x1,y1] # Coordinates of the textblocks . Used while checking relevance of image and text .\n",
    "\n",
    "                    text_blocks.append({\n",
    "                        \"block_id\":block_id,\n",
    "                        \"text\":text,\n",
    "                        \"bbox\":block_bbox,\n",
    "                        \"page\":page_num,\n",
    "                    }) # Used while appending metadata .\n",
    "\n",
    "                flush(page_num) # Since remaining text at end of the page may remain without being used so using flush to make a Document .\n",
    "\n",
    "            all_pdf_size+=pdf_size_mb\n",
    "            pdf.close()\n",
    "            print(\"-\"*45)\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading {pdf_path.name} . Error {e}\") # Exception handling .\n",
    "            failed_pdf.append(pdf_path.name) # Storing the PDF failed to load .\n",
    "            traceback.print_exc() # Used to trace failures similar to python interpreter stack trace .\n",
    "\n",
    "    # Some stats of Loading all the PDF in a directory .\n",
    "    print(f\"Total size of all the PDF's are : {all_pdf_size:.3f} MB\")\n",
    "    print(f\"Total Number of Documents : {len(all_documents)} \")\n",
    "    print(\"-\"*45)\n",
    "\n",
    "    # Printing all the PDF which where not able to load .\n",
    "    if failed_pdf:\n",
    "        print(f\"Failed PDF : {failed_pdf}\")\n",
    "\n",
    "    return all_documents # Returning the loaded Documents . Type -> List of Document"
   ],
   "id": "c7aaa5010c657e8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking",
   "id": "3474d299a61bb3cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.028356200Z",
     "start_time": "2026-01-26T16:52:15.013168500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document # Datatype of a block or a chunk .\n",
    "from typing import List # Used to store list of Documents or to specify return type ."
   ],
   "id": "3c85ba1031f9bca0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.051264Z",
     "start_time": "2026-01-26T16:52:15.031101800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is to calculate distance between two blocks and a threshold is set such that if two blocks are far those both blocks are separated with different chunks .\n",
    "def vertical_gap(block1,block2)->float:\n",
    "    return block2[\"bbox\"][1]-block1[\"bbox\"][3] # Distance between bottom of block 1 and top of block 2."
   ],
   "id": "9bfc4b8669097fbb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.075178800Z",
     "start_time": "2026-01-26T16:52:15.052402800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is used for getting outermost edge of all the chunks combined .\n",
    "def merge_bbox(blocks):\n",
    "    return(\n",
    "        min(b[\"bbox\"][0] for b in blocks), # x0 left\n",
    "        min(b[\"bbox\"][1] for b in blocks), # y0 top\n",
    "        max(b[\"bbox\"][2] for b in blocks), # x1 right\n",
    "        max(b[\"bbox\"][3] for b in blocks)  # y1 bottom1\n",
    "    )"
   ],
   "id": "84cfc3aded37d10f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.089206500Z",
     "start_time": "2026-01-26T16:52:15.076204800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is used create a chunk by adding\n",
    "def build_chunk(doc:Document,blocks:list,images:list)->Document:\n",
    "    chunk_text=\"\\n\".join(b[\"text\"] for b in blocks) # Combing all the texts from the blocks .\n",
    "    chunk_bbox=merge_bbox(blocks) # Used to get overall chunk coordinates .\n",
    "\n",
    "    return Document(\n",
    "        page_content=chunk_text,\n",
    "        metadata={\n",
    "            \"source\":doc.metadata[\"source\"],\n",
    "            \"page_num\":doc.metadata[\"page_num\"],\n",
    "            \"bbox\":chunk_bbox,\n",
    "            \"text_blocks\":blocks,\n",
    "            \"images\":images\n",
    "        }\n",
    "    ) # Adding metadata ."
   ],
   "id": "90e29ec1accb3b88",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.105075900Z",
     "start_time": "2026-01-26T16:52:15.090209400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is used to check if a image is relevant to a chunk using coordinates .\n",
    "def bbox_overlap(a,b)->bool: # Here a is chunk bbox and b is image bbox. (using a and b instead of chunk and img because we might use this function for something else in future code .)\n",
    "    return not(\n",
    "        a[2] < b[0] or # right edge of a and left edge of b (here we are considering a is at left and b is right side of a .)\n",
    "        a[0] > b[2] or # left edge of a and right edge of b (here we are considering b is at left and a is right side of b .)\n",
    "        a[3] < b[1] or # bottom edge of a and top edge of b (here we are considering a is at top and b is below of a .)\n",
    "        a[1] > b[3]    # top edge of a and bottom edge of b (here we are considering b is at top and a is below of b .)\n",
    "    )"
   ],
   "id": "6f08872a20190b83",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main chunking strategy",
   "id": "dab75cd804fe5907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.120187300Z",
     "start_time": "2026-01-26T16:52:15.106070700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is main chunking strategy , it uses bbox , max characters to chunk different blocks together .\n",
    "# max_chars -> maximum characters in a single chunk . (I guess we can replace with token based chunking using tiktoken need to check on that .)\n",
    "# max_vertical_gap -> maximum vertical height between two blocks . Calculated using bbox .\n",
    "\n",
    "def bbox_chunker(documents:List[Document],max_chars:int=1000,max_vertical_gap:int=40)->List[Document]:\n",
    "    all_chunks=[] # Used to store chunks .\n",
    "\n",
    "    for doc in documents:\n",
    "        blocks=doc.metadata.get(\"text_blocks\",[]) # Used to store text of a block .\n",
    "        images=doc.metadata.get(\"images\",[]) # Used to store all the images of a Document .\n",
    "\n",
    "        if not blocks:\n",
    "            continue\n",
    "\n",
    "        current_blocks=[] # Used to store blocks to store in a chunk .\n",
    "        current_len=0 # Calculating maximum characters in chunk .\n",
    "\n",
    "        for block in blocks:\n",
    "            text=block[\"text\"]\n",
    "            block_len=len(text) # Calculating characters in a single block .\n",
    "\n",
    "            if current_blocks:\n",
    "                gap=vertical_gap(current_blocks[-1],block) # Previous block and current block\n",
    "            else:\n",
    "                gap=0 # Basically first block of a Document .\n",
    "\n",
    "            if current_len+block_len>max_chars or gap> max_vertical_gap:\n",
    "                all_chunks.append(build_chunk(doc,current_blocks,images)) # Creating a chunk and appending it .\n",
    "                current_blocks=[]\n",
    "                current_len=0\n",
    "            current_blocks.append(block)\n",
    "            current_len+=block_len\n",
    "\n",
    "        if current_blocks:\n",
    "            all_chunks.append(\n",
    "                build_chunk(doc,current_blocks,images) # if any block is missed at end of document then it is chunked separately .\n",
    "            )\n",
    "    print(f\"{len(all_chunks)} of chunks were created using {len(documents)} documents .\")\n",
    "    return all_chunks"
   ],
   "id": "68a18bcbed289bd5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Image relevance strategy",
   "id": "c22cfc8dc18ca02c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:15.135633400Z",
     "start_time": "2026-01-26T16:52:15.121186100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function is used to attach images to relevant chunks .\n",
    "def attach_images(chunks:List[Document])->List[Document]:\n",
    "    for chunk in chunks: # Retrieving a chunk and its images .\n",
    "        chunk_bbox=chunk.metadata[\"bbox\"]\n",
    "        page_images=chunk.metadata[\"images\"]\n",
    "\n",
    "        # Checking relevance of image using bbox or coordinates .\n",
    "        relevant=[\n",
    "            img for img in page_images\n",
    "            if bbox_overlap(chunk_bbox,img[\"bbox\"]) # If images overlaps to the chunk then image is attached to chunk .\n",
    "        ]\n",
    "        chunk.metadata[\"images\"]=relevant # Adding images to metadata .\n",
    "\n",
    "    return chunks"
   ],
   "id": "d80628a1b67a5ccf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.687485100Z",
     "start_time": "2026-01-26T16:52:15.135633400Z"
    }
   },
   "cell_type": "code",
   "source": "documents=loading_pdf() # Loading PDF",
   "id": "e38745a4d0c0f0ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory path is : ..\\data\\pdf .\n",
      "Number of PDF's in directory is 1\n",
      "==================== PDF LOAD SUMMARY ====================\n",
      "---------------------------------------------\n",
      "1 ---> Loading hubble-science-highlights.pdf \n",
      "File size : 14.789 MB\n",
      "---------------------------------------------\n",
      "Total size of all the PDF's are : 14.789 MB\n",
      "Total Number of Documents : 74 \n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.786702300Z",
     "start_time": "2026-01-26T16:52:24.766147600Z"
    }
   },
   "cell_type": "code",
   "source": "chunks=bbox_chunker(documents) # Creating chunks .",
   "id": "9ae00de142189bc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 of chunks were created using 74 documents .\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.805350400Z",
     "start_time": "2026-01-26T16:52:24.787731800Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunks[1]) # Displaying a single chunk .",
   "id": "51b720fbb59f5139",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='HUBBLE SPACE TELESCOPE' metadata={'source': 'hubble-science-highlights.pdf', 'page_num': 2, 'bbox': (443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625), 'text_blocks': [{'block_id': 0, 'text': 'HUBBLE SPACE TELESCOPE', 'bbox': [443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625], 'page': 2}], 'images': [{'image_id': 'hubble-science-highlights_p2_i0', 'path': '..\\\\data\\\\images_pymupdf\\\\hubble-science-highlights\\\\page_2_img_0.png', 'page': 2, 'bbox': [39.327880859375, 25.74951171875, 581.6133422851562, 364.67791748046875]}]}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.827372900Z",
     "start_time": "2026-01-26T16:52:24.806363700Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunks[1].page_content) # Page content of a chunk .",
   "id": "2366090b189f3efc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUBBLE SPACE TELESCOPE\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.845749Z",
     "start_time": "2026-01-26T16:52:24.828743200Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunks[1].metadata) # Metadata of a chunk .",
   "id": "cdab50441b70a45f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'hubble-science-highlights.pdf', 'page_num': 2, 'bbox': (443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625), 'text_blocks': [{'block_id': 0, 'text': 'HUBBLE SPACE TELESCOPE', 'bbox': [443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625], 'page': 2}], 'images': [{'image_id': 'hubble-science-highlights_p2_i0', 'path': '..\\\\data\\\\images_pymupdf\\\\hubble-science-highlights\\\\page_2_img_0.png', 'page': 2, 'bbox': [39.327880859375, 25.74951171875, 581.6133422851562, 364.67791748046875]}]}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.858231300Z",
     "start_time": "2026-01-26T16:52:24.846750500Z"
    }
   },
   "cell_type": "code",
   "source": "final_chunks=attach_images(chunks) # Attaching images to relevant chunks .",
   "id": "bb28d20a71c46ca9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.884265500Z",
     "start_time": "2026-01-26T16:52:24.859230400Z"
    }
   },
   "cell_type": "code",
   "source": "print(final_chunks[1]) # Displaying a single chunk .",
   "id": "d4c8a6f8b108e5a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='HUBBLE SPACE TELESCOPE' metadata={'source': 'hubble-science-highlights.pdf', 'page_num': 2, 'bbox': (443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625), 'text_blocks': [{'block_id': 0, 'text': 'HUBBLE SPACE TELESCOPE', 'bbox': [443.36920166015625, 366.5845947265625, 565.1551513671875, 379.16461181640625], 'page': 2}], 'images': []}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.944288700Z",
     "start_time": "2026-01-26T16:52:24.885231500Z"
    }
   },
   "cell_type": "code",
   "source": "print(final_chunks[1].page_content) # Page content of a chunk .",
   "id": "9cf7bb2a9d264663",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUBBLE SPACE TELESCOPE\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:24.982288200Z",
     "start_time": "2026-01-26T16:52:24.944288700Z"
    }
   },
   "cell_type": "code",
   "source": "print(final_chunks[0].metadata) # Metadata of a chunk .",
   "id": "15b9462e4d0e0ab8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'hubble-science-highlights.pdf', 'page_num': 1, 'bbox': (175.6049041748047, 471.7757263183594, 503.6419677734375, 756.75), 'text_blocks': [{'block_id': 0, 'text': 'Reshaping Our \\nCosmic View', 'bbox': [218.0, 471.7757263183594, 503.6419677734375, 570.6857299804688], 'page': 1}, {'block_id': 1, 'text': 'H U B B L E  S P A C E  T E L E S C O P E', 'bbox': [175.6049041748047, 489.989990234375, 195.92990112304688, 756.75], 'page': 1}, {'block_id': 2, 'text': 'Hubble Science Highlights', 'bbox': [221.0, 570.6558837890625, 483.0419921875, 600.4658813476562], 'page': 1}], 'images': [{'image_id': 'hubble-science-highlights_p1_i0', 'path': '..\\\\data\\\\images_pymupdf\\\\hubble-science-highlights\\\\page_1_img_0.png', 'page': 1, 'bbox': [-1.0525050163269043, -1.1285400390625, 613.0826416015625, 793.124755859375]}]}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:25.009764300Z",
     "start_time": "2026-01-26T16:52:24.983300300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Used for json data manipulation .\n",
    "import json"
   ],
   "id": "d69c85763a1f7c16",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:52:25.039596400Z",
     "start_time": "2026-01-26T16:52:25.010763Z"
    }
   },
   "cell_type": "code",
   "source": "print(json.dumps(final_chunks[0].metadata,indent=2,sort_keys=True)) # Just a pretty print of the metadata .",
   "id": "a9f44f08a2b48801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bbox\": [\n",
      "    175.6049041748047,\n",
      "    471.7757263183594,\n",
      "    503.6419677734375,\n",
      "    756.75\n",
      "  ],\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"bbox\": [\n",
      "        -1.0525050163269043,\n",
      "        -1.1285400390625,\n",
      "        613.0826416015625,\n",
      "        793.124755859375\n",
      "      ],\n",
      "      \"image_id\": \"hubble-science-highlights_p1_i0\",\n",
      "      \"page\": 1,\n",
      "      \"path\": \"..\\\\data\\\\images_pymupdf\\\\hubble-science-highlights\\\\page_1_img_0.png\"\n",
      "    }\n",
      "  ],\n",
      "  \"page_num\": 1,\n",
      "  \"source\": \"hubble-science-highlights.pdf\",\n",
      "  \"text_blocks\": [\n",
      "    {\n",
      "      \"bbox\": [\n",
      "        218.0,\n",
      "        471.7757263183594,\n",
      "        503.6419677734375,\n",
      "        570.6857299804688\n",
      "      ],\n",
      "      \"block_id\": 0,\n",
      "      \"page\": 1,\n",
      "      \"text\": \"Reshaping Our \\nCosmic View\"\n",
      "    },\n",
      "    {\n",
      "      \"bbox\": [\n",
      "        175.6049041748047,\n",
      "        489.989990234375,\n",
      "        195.92990112304688,\n",
      "        756.75\n",
      "      ],\n",
      "      \"block_id\": 1,\n",
      "      \"page\": 1,\n",
      "      \"text\": \"H U B B L E  S P A C E  T E L E S C O P E\"\n",
      "    },\n",
      "    {\n",
      "      \"bbox\": [\n",
      "        221.0,\n",
      "        570.6558837890625,\n",
      "        483.0419921875,\n",
      "        600.4658813476562\n",
      "      ],\n",
      "      \"block_id\": 2,\n",
      "      \"page\": 1,\n",
      "      \"text\": \"Hubble Science Highlights\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
